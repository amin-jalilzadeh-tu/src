{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.9-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Downloading psycopg2-2.9.9-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.2 MB 991.0 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.1/1.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.3/1.2 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.4/1.2 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.6/1.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.7/1.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.8/1.2 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.9/1.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.1/1.2 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "def get_db_config():\n",
    "    return {\n",
    "        \"dbname\": os.getenv('DB_NAME', \"ESDL_DB\"),\n",
    "        \"user\": os.getenv('DB_USER', \"postgres\"),\n",
    "        \"password\": os.getenv('DB_PASSWORD', \"mypassword\"),\n",
    "        \"host\": os.getenv('DB_HOST', \"leda.geodan.nl\"),\n",
    "        \"port\": os.getenv('DB_PORT', \"5432\")\n",
    "    }\n",
    "\n",
    "def connect_to_db():\n",
    "    config = get_db_config()\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config)\n",
    "        return conn\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error while connecting to PostgreSQL\", error)\n",
    "        return None\n",
    "\n",
    "def get_schema_table_column_data():\n",
    "    conn = connect_to_db()\n",
    "    if conn is not None:\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            # Open a CSV file to write the data into\n",
    "            with open('D:/database_columns_with_samples_ESDL_DB.csv', 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                # Write CSV Header\n",
    "                writer.writerow(['Schema Name', 'Table Name', 'Column Name', 'Data Type', 'Column Description', 'Sample Data 1', 'Sample Data 2', 'Sample Data 3'])\n",
    "\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT n.nspname AS schema_name, \n",
    "                           c.relname AS table_name, \n",
    "                           a.attname AS column_name, \n",
    "                           pg_catalog.format_type(a.atttypid, a.atttypmod) AS data_type,\n",
    "                           pg_catalog.col_description(a.attrelid, a.attnum) AS column_description\n",
    "                    FROM pg_catalog.pg_attribute a\n",
    "                    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid\n",
    "                    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid\n",
    "                    WHERE a.attnum > 0 AND NOT a.attisdropped AND c.relkind = 'r'\n",
    "                          AND n.nspname NOT IN ('pg_catalog', 'information_schema')\n",
    "                    ORDER BY n.nspname, c.relname, a.attnum;\n",
    "                \"\"\")\n",
    "                rows = cur.fetchall()\n",
    "                for row in rows:\n",
    "                    schema_name = f'\"{row[0]}\"'\n",
    "                    table_name = f'\"{row[1]}\"'\n",
    "                    column_name = f'\"{row[2]}\"'\n",
    "                    # Try to fetch sample data for each column\n",
    "                    try:\n",
    "                        cur.execute(f\"SELECT {column_name} FROM {schema_name}.{table_name} LIMIT 3\")\n",
    "                        samples = cur.fetchall()\n",
    "                    except psycopg2.Error as e:\n",
    "                        samples = [('Error',), ('Error',), ('Error',)]\n",
    "                        print(f\"Error fetching samples for {schema_name}.{table_name}.{column_name}: {e}\")\n",
    "                    \n",
    "                    # Flatten the samples into a single row\n",
    "                    sample_data = [sample[0] for sample in samples] + [''] * (3 - len(samples))\n",
    "                    writer.writerow(list(row) + sample_data)\n",
    "            cur.close()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            print(\"Error:\", error)\n",
    "        finally:\n",
    "            conn.close()\n",
    "    else:\n",
    "        print(\"Connection to the database failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_schema_table_column_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE all_databases_columns (\n",
    "    database_name TEXT,\n",
    "    schema_name TEXT,\n",
    "    table_name TEXT,\n",
    "    column_name TEXT,\n",
    "    data_type TEXT,\n",
    "    column_description TEXT,\n",
    "    sample_data_1 TEXT,\n",
    "    sample_data_2 TEXT,\n",
    "    sample_data_3 TEXT\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def get_db_config():\n",
    "    return {\n",
    "        \"dbname\": os.getenv('DB_NAME', \"Dataless\"),  # Name of your target database\n",
    "        \"user\": os.getenv('DB_USER', \"postgres\"),\n",
    "        \"password\": os.getenv('DB_PASSWORD', \"mypassword\"),\n",
    "        \"host\": os.getenv('DB_HOST', \"leda.geodan.nl\"),\n",
    "        \"port\": os.getenv('DB_PORT', \"5432\")\n",
    "    }\n",
    "\n",
    "def connect_to_db():\n",
    "    config = get_db_config()\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config)\n",
    "        return conn\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error while connecting to PostgreSQL\", error)\n",
    "        return None\n",
    "\n",
    "def import_csv_to_db(filename, dbname):\n",
    "    conn = connect_to_db()\n",
    "    if conn is not None:\n",
    "        try:\n",
    "            # Set a more reasonable limit for CSV field size\n",
    "            csv.field_size_limit(1000000000)  # Set to 1MB\n",
    "            cur = conn.cursor()\n",
    "            with open(filename, 'r') as f:\n",
    "                next(f)  # Skip the header row\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    # Insert each row into the database, prepend the dbname to the row data\n",
    "                    cur.execute(\"\"\"\n",
    "                        INSERT INTO all_databases_columns (database_name, schema_name, table_name, column_name, data_type, column_description, sample_data_1, sample_data_2, sample_data_3)\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                    \"\"\", [dbname] + row)\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            print(\"Error:\", error)\n",
    "        finally:\n",
    "            conn.close()\n",
    "    else:\n",
    "        print(\"Connection to the database failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of CSV files and their corresponding database names\n",
    "    csv_files = {\n",
    "        \"D:\\\\database_columns_with_samples_Pico.csv\": \"Pico\",\n",
    "        \"D:\\\\database_columns_with_samples_rws.csv\": \"rws\",\n",
    "        \"D:\\\\database_columns_with_samples_research.csv\": \"research\",\n",
    "        \"D:\\\\database_columns_with_samples_postcode.csv\": \"postcode\",\n",
    "        \"D:\\\\database_columns_with_samples_maquette_nl.csv\": \"maquette_nl\",\n",
    "        \"D:\\\\database_columns_with_samples_VU.csv\": \"VU\",\n",
    "        \"D:\\\\database_columns_with_samples_ESDL_DB.csv\": \"ESDL_DB\"\n",
    "    }\n",
    "    \n",
    "    for file, dbname in csv_files.items():\n",
    "        import_csv_to_db(file, dbname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE INDEX idx_column_name ON all_databases_columns (column_name);\n",
    "\n",
    "CREATE INDEX idx_column_name_trgm ON all_databases_columns USING GIN (column_name gin_trgm_ops);\n",
    "\n",
    "\n",
    "\n",
    "-- Add a new column to store the text search vector\n",
    "ALTER TABLE all_databases_columns ADD COLUMN tsvector_col tsvector;\n",
    "\n",
    "-- Update the new column with vectorized text search data\n",
    "UPDATE all_databases_columns\n",
    "SET tsvector_col = to_tsvector('english', coalesce(column_name, ''));\n",
    "\n",
    "-- Create an index on the tsvector column for faster search\n",
    "CREATE INDEX tsvector_col_idx ON all_databases_columns USING GIN (tsvector_col);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-- Search for entries that match 'example query'\n",
    "SELECT * FROM all_databases_columns\n",
    "WHERE tsvector_col @@ to_tsquery('english', 'example & query');\n",
    "\n",
    "\n",
    "\n",
    "SELECT * FROM all_databases_columns\n",
    "WHERE tsvector_col @@ to_tsquery('english', 'ba:*');\n",
    "\n",
    "\n",
    "SELECT * FROM all_databases_columns\n",
    "WHERE column_description ILIKE '%ba%';\n",
    "\n",
    "\n",
    "SELECT * FROM all_databases_columns\n",
    "WHERE tsvector_col @@ to_tsquery('english', 'bag') AND column_description ILIKE '%ba%';\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using Prefix Matching in Full-Text Search\n",
    "PostgreSQL allows prefix matching in full-text searches by using the :* operator, which can be helpful if you're looking for words that start with certain letters:\n",
    "\n",
    "sql\n",
    "Copy code\n",
    "SELECT * FROM all_databases_columns\n",
    "WHERE tsvector_col @@ to_tsquery('english', 'ba:*');\n",
    "This query will match any entries where words start with \"ba\" (like \"bag\", \"bat\", \"batter\", etc.). However, this only works for prefixes, not arbitrary substring searching.\n",
    "\n",
    "2. Using ILIKE or LIKE for Substring Searches\n",
    "If you need to perform arbitrary substring searches (not just prefixes), and you're okay with a potentially slower search for large datasets without the benefits of full-text indexing, you can use the LIKE or ILIKE operators:\n",
    "\n",
    "sql\n",
    "Copy code\n",
    "SELECT * FROM all_databases_columns\n",
    "WHERE column_description ILIKE '%ba%';\n",
    "The ILIKE operator performs a case-insensitive search, matching any part of the string. This is less efficient than full-text search because it does not use an index in the same way, but it can find any occurrence of the substring \"ba\" anywhere in the column_description field.\n",
    "\n",
    "3. Combining Full-Text and LIKE Searches\n",
    "For scenarios where you need the benefits of full-text search but also need to catch more granular substring matches, you might combine full-text search with LIKE:\n",
    "\n",
    "sql\n",
    "Copy code\n",
    "SELECT * FROM all_databases_columns\n",
    "WHERE tsvector_col @@ to_tsquery('english', 'bag') AND column_description ILIKE '%ba%';\n",
    "This approach uses full-text search to quickly narrow down the results using the efficient tsvector index and then further filters those results with a substring match. This can be useful for reducing the performance cost of LIKE in very large datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SELECT * FROM all_databases_columns WHERE column_name ILIKE '%....%';\n",
    "\n",
    "SELECT * FROM all_databases_columns WHERE sample_data_1 ILIKE '%woning%';\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
